{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afc570bf-e129-415b-8f2d-8bbce08131ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr\n",
    "% pip install -U langgraph langchain-community langchain-anthropic tavily-python pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "358e5666-b7c5-4e46-90a1-7ea273d86ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"pr-overcooked-yak-71\"\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = (\n",
    "    \"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\"\n",
    ")\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = (\n",
    "    \"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\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb365e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"ANTHROPIC_API_KEY\")\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0dce432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "DIFY_BASE_URL = \"http://dify.dev.byteroll.ai\"\n",
    "CMS_KNOWLEDGE_BASE_ID = \"ba15c23a-31a4-4794-a825-0bdac51f59fb\"\n",
    "NPI_KNOWLEDGE_BASE_ID = \"ba15c23a-31a4-4794-a825-0bdac51f59fb\"\n",
    "DIFY_API_KEY = \"dataset-xxpigKLHGWSraHCt52Q2do20\"\n",
    "\n",
    "@tool\n",
    "def npi_lookup(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Query the Dify knowledge base for relevant documents using the /retrieve endpoint.\n",
    "    Returns the top results combined into a single string.\n",
    "    \"\"\"\n",
    "    url = f\"{DIFY_BASE_URL}/v1/datasets/{NPI_KNOWLEDGE_BASE_ID}/retrieve\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {DIFY_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"query\": query,\n",
    "        \"retrieval_model\": {\n",
    "            \"search_method\": \"hybrid_search\",      # choose from: keyword_search, semantic_search, full_text_search, hybrid_search\n",
    "            \"reranking_enable\": False,              # False if reranking not needed\n",
    "            \"reranking_mode\": None,                 # null equivalent in Python is None\n",
    "            \"reranking_model\": {\n",
    "                \"reranking_provider_name\": \"\",\n",
    "                \"reranking_model_name\": \"\"\n",
    "            },\n",
    "            \"weights\": 0.7,                        # null equivalent in Python is None\n",
    "            \"top_k\": 3,                             # number of results to return\n",
    "            \"score_threshold_enabled\": False,       # disable score threshold\n",
    "            \"score_threshold\": None                 # null equivalent\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    records = data.get(\"records\", [])\n",
    "    contents = []\n",
    "    for record in records:\n",
    "        segment = record.get(\"segment\", {})\n",
    "        content = segment.get(\"content\", \"\")\n",
    "        if content:\n",
    "            contents.append(content.strip())\n",
    "\n",
    "    return \"\\n\\n\".join(contents)\n",
    "\n",
    "@tool\n",
    "def cms_lookup(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Query the Dify knowledge base for relevant documents using the /retrieve endpoint.\n",
    "    Returns the top results combined into a single string.\n",
    "    \"\"\"\n",
    "    url = f\"{DIFY_BASE_URL}/v1/datasets/{CMS_KNOWLEDGE_BASE_ID}/retrieve\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {DIFY_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"query\": query,\n",
    "        \"retrieval_model\": {\n",
    "            \"search_method\": \"hybrid_search\",      # choose from: keyword_search, semantic_search, full_text_search, hybrid_search\n",
    "            \"reranking_enable\": False,              # False if reranking not needed\n",
    "            \"reranking_mode\": None,                 # null equivalent in Python is None\n",
    "            \"reranking_model\": {\n",
    "                \"reranking_provider_name\": \"\",\n",
    "                \"reranking_model_name\": \"\"\n",
    "            },\n",
    "            \"weights\": 0.7,                        # null equivalent in Python is None\n",
    "            \"top_k\": 3,                             # number of results to return\n",
    "            \"score_threshold_enabled\": False,       # disable score threshold\n",
    "            \"score_threshold\": None                 # null equivalent\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    records = data.get(\"records\", [])\n",
    "    contents = []\n",
    "    for record in records:\n",
    "        segment = record.get(\"segment\", {})\n",
    "        content = segment.get(\"content\", \"\")\n",
    "        if content:\n",
    "            contents.append(content.strip())\n",
    "\n",
    "    return \"\\n\\n\".join(contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f08190c-21f6-4a07-b9e2-3aa991fe4eed",
   "metadata": {},
   "source": [
    "#### Excursions\n",
    "\n",
    "Finally, define some tools to let the user search for things to do (and make reservations) once they arrive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf5d064",
   "metadata": {},
   "source": [
    "#### Utilities\n",
    "\n",
    "Define helper functions to pretty print the messages in the graph while we debug it and to give our tool node error handling (by adding the error to the chat history).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "663f001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "def handle_tool_error(state) -> dict:\n",
    "    error = state.get(\"error\")\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",\n",
    "                tool_call_id=tc[\"id\"],\n",
    "            )\n",
    "            for tc in tool_calls\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def create_tool_node_with_fallback(tools: list) -> dict:\n",
    "    return ToolNode(tools).with_fallbacks(\n",
    "        [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _print_event(event: dict, _printed: set, max_length=1500):\n",
    "    current_state = event.get(\"dialog_state\")\n",
    "    if current_state:\n",
    "        print(\"Currently in: \", current_state[-1])\n",
    "    message = event.get(\"messages\")\n",
    "    if message:\n",
    "        if isinstance(message, list):\n",
    "            message = message[-1]\n",
    "        if message.id not in _printed:\n",
    "            msg_repr = message.pretty_repr(html=True)\n",
    "            # if len(msg_repr) > max_length:\n",
    "            #     msg_repr = msg_repr[:max_length] + \" ... (truncated)\"\n",
    "            print(msg_repr)\n",
    "            _printed.add(message.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a578528",
   "metadata": {},
   "source": [
    "### Agent State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2997e1f9-3a4b-4794-b71f-992da3a644fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, Optional\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "\n",
    "def update_dialog_stack(left: list[str], right: Optional[str]) -> list[str]:\n",
    "    \"\"\"Push or pop the state.\"\"\"\n",
    "    if right is None:\n",
    "        return left\n",
    "    if right == \"pop\":\n",
    "        return left[:-1]\n",
    "    return left + [right]\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    user_info: str\n",
    "    dialog_state: Annotated[\n",
    "        list[\n",
    "            Literal[\n",
    "                \"assistant\", # main orchestrator\n",
    "                \"get_analytics\", # get analytics\n",
    "                \"qualify_leads\", # qualify leads\n",
    "                \"procpect_leads\", # prospect leads\n",
    "                \"get_strategy\",  # get strategy\n",
    "            ]\n",
    "        ],\n",
    "        update_dialog_stack,\n",
    "    ]\n",
    "    \n",
    "# Haiku is faster and cheaper, but less accurate\n",
    "# llm = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "llm = ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=1)\n",
    "# You could swap LLMs, though you will likely want to update the prompts when\n",
    "# doing so!\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67fb372-a8dc-49c8-b86a-2742e0f8aae9",
   "metadata": {},
   "source": [
    "#### Assistants\n",
    "\n",
    "This time we will create an assistant **for every workflow**. That means:\n",
    "\n",
    "1. Flight booking assistant\n",
    "2. Hotel booking assistant\n",
    "3. Car rental assistant\n",
    "4. Excursion assistant\n",
    "5. and finally, a \"primary assistant\" to route between these\n",
    "\n",
    "If you're paying attention, you may recognize this as an example of the **supervisor** design pattern from our Multi-agent examples.\n",
    "\n",
    "Below, define the `Runnable` objects to power each assistant.\n",
    "Each `Runnable` has a prompt, LLM, and schemas for the tools scoped to that assistant.\n",
    "Each _specialized_ / delegated assistant additionally can call the `CompleteOrEscalate` tool to indicate that the control flow should be passed back to the primary assistant. This happens if it has successfully completed its work or if the user has changed their mind or needs assistance on something that beyond the scope of that particular workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46da448-a1fc-4309-80db-4b7feed9f96b",
   "metadata": {},
   "source": [
    "<div class=\"admonition note\">\n",
    "    <p class=\"admonition-title\">Using Pydantic with LangChain</p>\n",
    "    <p>\n",
    "        This notebook uses Pydantic v2 <code>BaseModel</code>, which requires <code>langchain-core >= 0.3</code>. Using <code>langchain-core < 0.3</code> will result in errors due to mixing of Pydantic v1 and v2 <code>BaseModels</code>.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ef67c85-b999-406c-a745-09fdc0dfa0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable, RunnableConfig\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Assistant:\n",
    "    def __init__(self, runnable: Runnable):\n",
    "        self.runnable = runnable\n",
    "\n",
    "    def __call__(self, state: State, config: RunnableConfig):\n",
    "        while True:\n",
    "            result = self.runnable.invoke(state)\n",
    "\n",
    "            if not result.tool_calls and (\n",
    "                not result.content\n",
    "                or isinstance(result.content, list)\n",
    "                and not result.content[0].get(\"text\")\n",
    "            ):\n",
    "                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
    "                state = {**state, \"messages\": messages}\n",
    "            else:\n",
    "                break\n",
    "        return {\"messages\": result}\n",
    "\n",
    "\n",
    "class CompleteOrEscalate(BaseModel):\n",
    "    \"\"\"A tool to mark the current task as completed and/or to escalate control of the dialog to the main assistant,\n",
    "    who can re-route the dialog based on the user's needs.\"\"\"\n",
    "\n",
    "    cancel: bool = True\n",
    "    reason: str\n",
    "\n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"cancel\": True,\n",
    "                \"reason\": \"User changed their mind about the current task.\",\n",
    "            },\n",
    "            \"example 2\": {\n",
    "                \"cancel\": True,\n",
    "                \"reason\": \"I have fully completed the task.\",\n",
    "            },\n",
    "            \"example 3\": {\n",
    "                \"cancel\": False,\n",
    "                \"reason\": \"I need to search the database for more information.\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "\n",
    "# Analytics assistant\n",
    "\n",
    "anlaytics_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a specialized **Analytics Assistant** for healthcare data analysis and insights delivery. \"\n",
    "            \"The main assistant delegates tasks to you when the user requires insights, trends, or analysis of healthcare data. \"\n",
    "            \"Your role includes analyzing healthcare datasets, summarizing trends, identifying patterns, and visualizing insights.\\n\\n\"\n",
    "            \"### Instructions:\\n\"\n",
    "            \"1. Analyze user queries and determine the type of analysis required (trends, benchmarks, outlier detection, etc.).\\n\"\n",
    "            \"2. Use tools to retrieve relevant healthcare data and generate summaries, insights, or visualizations.\\n\"\n",
    "            \"3. Present findings in a concise and structured format using clear headings and bullet points.\\n\\n\"\n",
    "            \"### Capabilities:\\n\"\n",
    "            \"- Generate descriptive statistics for datasets.\\n\"\n",
    "            \"- Perform trend analysis over time.\\n\"\n",
    "            \"- Create visualizations (line charts, bar charts) for insights.\\n\"\n",
    "            \"- Summarize categorical data.\\n\\n\"\n",
    "            \"### Escalation:\\n\"\n",
    "            \"If additional tools are required or the user's needs go beyond data analysis, escalate the task to the main assistant \"\n",
    "            'using \"CompleteOrEscalate\".\\n\\n'\n",
    "            \"**Example Escalations:**\\n\"\n",
    "            \"- The user requests strategy recommendations based on the analysis.\\n\"\n",
    "            \"- The user changes the task to lead qualification.\\n\\n\"\n",
    "            \"### Current Time: {time}\"\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ").partial(time=datetime.now())\n",
    "\n",
    "\n",
    "update_anlaytics_tools = [cms_lookup, npi_lookup]\n",
    "update_anlaytics_runnable = anlaytics_prompt | llm.bind_tools(\n",
    "    update_anlaytics_tools + [CompleteOrEscalate]\n",
    ")\n",
    "\n",
    "# Lead qualification Assistant\n",
    "\n",
    "lead_qualification_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a specialized **Lead Qualification Assistant** responsible for evaluating healthcare leads. \"\n",
    "            \"The main assistant delegates tasks to you when the user needs to assess the quality or relevance of identified leads. \"\n",
    "            \"Your role is to qualify leads based on specified criteria and provide a confidence score or summary.\\n\\n\"\n",
    "            \"### Instructions:\\n\"\n",
    "            \"1. Analyze the provided leads to determine their fit for the user's goals (e.g., relevance, role, organization).\\n\"\n",
    "            \"2. Assess and prioritize leads based on user-provided criteria.\\n\"\n",
    "            \"3. Return a structured qualification summary, including:\\n\"\n",
    "            \"   - Lead Name\\n\"\n",
    "            \"   - Qualification Status (High, Medium, Low)\\n\"\n",
    "            \"   - Notes explaining the reasoning.\\n\\n\"\n",
    "            \"### Capabilities:\\n\"\n",
    "            \"- Qualify leads based on user criteria.\\n\"\n",
    "            \"- Provide confidence scores and prioritization.\\n\"\n",
    "            \"- Escalate tasks if leads require further analysis or strategic planning.\\n\\n\"\n",
    "            \"### Escalation:\\n\"\n",
    "            'If further analysis, prospecting, or strategy planning is required, escalate using \"CompleteOrEscalate\".\\n\\n'\n",
    "            \"**Example Escalations:**\\n\"\n",
    "            \"- User requests outreach strategies for the qualified leads.\\n\"\n",
    "            \"- User changes the focus to trend analysis.\\n\\n\"\n",
    "            \"### Current Time: {time}\"\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ").partial(time=datetime.now())\n",
    "\n",
    "\n",
    "book_lead_qualification_tools = [cms_lookup, npi_lookup]\n",
    "lead_qualification_runnable = lead_qualification_prompt | llm.bind_tools(\n",
    "    book_lead_qualification_tools + [CompleteOrEscalate]\n",
    ")\n",
    "\n",
    "# Prospecting Assistant\n",
    "\n",
    "prospecting_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a specialized **Prospecting Assistant** focused on identifying healthcare leads. \"\n",
    "            \"The main assistant delegates tasks to you when the user requests healthcare provider leads or contact information. \"\n",
    "            \"Your role is to find, extract, and organize leads based on the user's query.\\n\\n\"\n",
    "            \"### Instructions:\\n\"\n",
    "            \"1. Interpret user queries to determine lead criteria (e.g., roles, organizations, locations).\\n\"\n",
    "            \"2. Search the available databases for healthcare providers or decision-makers.\\n\"\n",
    "            \"3. Return structured lead information, including:\\n\"\n",
    "            \"   - Name\\n\"\n",
    "            \"   - Title/Role\\n\"\n",
    "            \"   - Organization\\n\\n\"\n",
    "            \"### Capabilities:\\n\"\n",
    "            \"- Search NPI or CMS data for healthcare providers.\\n\"\n",
    "            \"- Filter and organize leads based on relevance.\\n\"\n",
    "            \"- Escalate when leads require further qualification.\\n\\n\"\n",
    "            \"### Escalation:\\n\"\n",
    "            'If the task requires further lead qualification or strategy planning, escalate using \"CompleteOrEscalate\".\\n\\n'\n",
    "            \"**Example Escalations:**\\n\"\n",
    "            \"- User asks for lead qualification based on lead relevance.\\n\"\n",
    "            \"- User requests a marketing strategy for the identified leads.\\n\\n\"\n",
    "            \"### Current Time: {time}\"\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ").partial(time=datetime.now())\n",
    "\n",
    "prospecting_tools = [cms_lookup, npi_lookup]\n",
    "prospecting_runnable = prospecting_prompt | llm.bind_tools(\n",
    "    prospecting_tools + [CompleteOrEscalate]\n",
    ")\n",
    "\n",
    "# Strategy Assistant\n",
    "\n",
    "strategy_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a specialized **Strategy Planner Assistant** focused on generating outreach and marketing strategies. \"\n",
    "            \"The main assistant delegates tasks to you when the user needs strategic recommendations for healthcare leads or insights.\\n\\n\"\n",
    "            \"### Instructions:\\n\"\n",
    "            \"1. Use provided insights, qualified leads, and user goals to develop an actionable strategy.\\n\"\n",
    "            \"2. Break the strategy into clear steps, including:\\n\"\n",
    "            \"   - Target Audience\\n\"\n",
    "            \"   - Outreach Channels (e.g., email, LinkedIn)\\n\"\n",
    "            \"   - Key Messaging\\n\"\n",
    "            \"   - Call-to-Action\\n\\n\"\n",
    "            \"3. Present the strategy in a structured format using headings, bullet points, and clear summaries.\\n\\n\"\n",
    "            \"### Capabilities:\\n\"\n",
    "            \"- Create personalized outreach strategies.\\n\"\n",
    "            \"- Recommend marketing channels and messaging.\\n\"\n",
    "            \"- Escalate if additional data analysis or lead updates are needed.\\n\\n\"\n",
    "            \"### Escalation:\\n\"\n",
    "            'If additional data, lead identification, or further qualification is required, escalate using \"CompleteOrEscalate\".\\n\\n'\n",
    "            \"**Example Escalations:**\\n\"\n",
    "            \"- User requests a trend analysis before planning the strategy.\\n\"\n",
    "            \"- User changes the task to finding new leads.\\n\\n\"\n",
    "            \"### Current Time: {time}\"\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ").partial(time=datetime.now())\n",
    "\n",
    "strategy_tools = [cms_lookup, npi_lookup]\n",
    "strategy_runnable = strategy_prompt | llm.bind_tools(\n",
    "    strategy_tools + [CompleteOrEscalate]\n",
    ")\n",
    "\n",
    "\n",
    "# Primary Assistant\n",
    "class ToAnalyticsAssistant(BaseModel):\n",
    "    \"\"\"Transfers work to a specialized assistant to handle healthcare analytics.\"\"\"\n",
    "\n",
    "    request: str = Field(\n",
    "        description=\"Any necessary followup questions the update analytics assistant should clarify before proceeding.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ToLeadQualification(BaseModel):\n",
    "    \"\"\"Transfers work to a specialized assistant to handle lead qualification.\"\"\"\n",
    "\n",
    "\n",
    "    request: str = Field(\n",
    "        description=\"Any additional information or requests from the user regarding the lead qualification.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ToProspectingAssistant(BaseModel):\n",
    "    \"\"\"Transfer work to a specialized assistant to handle prospecting leads.\"\"\"\n",
    "\n",
    "    request: str = Field(\n",
    "        description=\"Any additional information or requests from the user regarding the prospecting leads.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ToStrategyAssistant(BaseModel):\n",
    "    \"\"\"Transfers work to a specialized assistant to handle strategy planning.\"\"\"\n",
    "\n",
    "    request: str = Field(\n",
    "        description=\"Any additional information or requests from the user regarding strategy planning.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# The top-level assistant performs general Q&A and delegates specialized tasks to other assistants.\n",
    "# The task delegation is a simple form of semantic routing / does simple intent detection\n",
    "# llm = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "llm = ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=1)\n",
    "\n",
    "primary_assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are the **Primary Orchestration Assistant**, responsible for coordinating tasks across specialized agents. \"\n",
    "            \"You handle user queries and delegate tasks to the appropriate assistant based on the user's intent. \"\n",
    "            \"You do not perform these tasks directly; instead, you quietly invoke specialized agents without mentioning them to the user.\\n\\n\"\n",
    "            \n",
    "            \"### Specialized Agents:\\n\"\n",
    "            \"1. **Analytics Assistant**: Handles data analysis, insights, trends, and visualizations.\\n\"\n",
    "            \"2. **Prospecting Assistant**: Finds and provides healthcare leads.\\n\"\n",
    "            \"3. **Lead Qualification Assistant**: Qualifies and evaluates leads for relevance.\\n\"\n",
    "            \"4. **Strategy Planner Assistant**: Generates outreach and marketing strategies based on leads or insights.\\n\\n\"\n",
    "            \n",
    "            \"### Instructions:\\n\"\n",
    "            \"1. Analyze the user's query to determine the task type:\\n\"\n",
    "            \"   - **Analytics**: Queries about data insights, trends, or analysis.\\n\"\n",
    "            \"   - **Prospecting**: Requests to find healthcare leads or contacts.\\n\"\n",
    "            \"   - **Lead Qualification**: Tasks involving lead evaluation or prioritization.\\n\"\n",
    "            \"   - **Strategy Planning**: Requests for outreach strategies or recommendations.\\n\\n\"\n",
    "            \"2. Delegate the task to the appropriate specialized assistant using the corresponding tool:\\n\"\n",
    "            \"   - `ToAnalyticsAssistant` for data analysis tasks.\\n\"\n",
    "            \"   - `ToProspectingAssistant` for lead identification.\\n\"\n",
    "            \"   - `ToLeadQualification` for lead qualification.\\n\"\n",
    "            \"   - `ToStrategyAssistant` for strategy planning.\\n\\n\"\n",
    "            \n",
    "            \"3. If a user query is unrelated to these tasks, respond with general information or escalate if needed.\\n\\n\"\n",
    "            \n",
    "            \"### Escalation:\\n\"\n",
    "            \"If a specialized assistant cannot handle the task (e.g., tool limitations or user changing focus), they will escalate the task back to you. \"\n",
    "            \"You must re-assess the user's query and re-route it appropriately.\\n\\n\"\n",
    "            \n",
    "            \"### Guidelines:\\n\"\n",
    "            \"- **Be Persistent**: If searches or tasks return no results initially, expand your scope before giving up.\\n\"\n",
    "            \"- **Do Not Reveal Agents**: The user should not be aware of the specialized assistants. Present results as if they came from you.\\n\"\n",
    "            \"- **Accuracy**: Double-check all outputs and databases before concluding that information is unavailable.\\n\"\n",
    "            \"- **Escalate Appropriately**: If tools cannot resolve the query, gracefully escalate to avoid wasting the user's time.\\n\\n\"\n",
    "            \n",
    "            \"### Examples of Routing:\\n\"\n",
    "            \"1. **User**: 'Can you analyze trends in patient admissions for last year?'\\n\"\n",
    "            \"   **Action**: Use `ToAnalyticsAssistant`.\\n\\n\"\n",
    "            \"2. **User**: 'Find procurement heads in hospitals around Texas.'\\n\"\n",
    "            \"   **Action**: Use `ToProspectingAssistant`.\\n\\n\"\n",
    "            \"3. **User**: 'Which of these leads are most relevant for our sales team?'\\n\"\n",
    "            \"   **Action**: Use `ToLeadQualification`.\\n\\n\"\n",
    "            \"4. **User**: 'Create a strategy for reaching out to hospital CEOs.'\\n\"\n",
    "            \"   **Action**: Use `ToStrategyAssistant`.\\n\\n\"\n",
    "            \"### Current User Context:\\n\"\n",
    "            \"<User_Info>\\n{user_info}\\n</User_Info>\\n\\n\"\n",
    "            \"### Current Time: {time}\"\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ").partial(time=datetime.now())\n",
    "\n",
    "primary_assistant_tools = [\n",
    "    TavilySearchResults(max_results=1),\n",
    "]\n",
    "assistant_runnable = primary_assistant_prompt | llm.bind_tools(\n",
    "    primary_assistant_tools\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6fc3e3-185f-4c1b-a2e3-cebac35ce0d6",
   "metadata": {},
   "source": [
    "#### Create Assistant\n",
    "\n",
    "We're about ready to create the graph. In the previous section, we made the design decision to have a shared `messages` state between all the nodes. This is powerful in that each delegated assistant can see the entire user journey and have a shared context. This, however, means that weaker LLMs can easily get mixed up about there specific scope. To mark the \"handoff\" between the primary assistant and one of the delegated workflows (and complete the tool call from the router), we will add a `ToolMessage` to the state.\n",
    "\n",
    "#### Utility\n",
    "\n",
    "Create a function to make an \"entry\" node for each workflow, stating \"the current assistant ix `assistant_name`\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb812818-99c9-4bf3-b1e5-a394c7b9058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "\n",
    "def create_entry_node(assistant_name: str, new_dialog_state: str) -> Callable:\n",
    "    def entry_node(state: State) -> dict:\n",
    "        tool_call_id = state[\"messages\"][-1].tool_calls[0][\"id\"]\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                ToolMessage(\n",
    "                    content=f\"The assistant is now the {assistant_name}. Reflect on the above conversation between the host assistant and the user.\"\n",
    "                    f\" The user's intent is unsatisfied. Use the provided tools to assist the user. Remember, you are {assistant_name},\"\n",
    "                    \" and the booking, update, other other action is not complete until after you have successfully invoked the appropriate tool.\"\n",
    "                    \" If the user changes their mind or needs help for other tasks, call the CompleteOrEscalate function to let the primary host assistant take control.\"\n",
    "                    \" Do not mention who you are - just act as the proxy for the assistant.\",\n",
    "                    tool_call_id=tool_call_id,\n",
    "                )\n",
    "            ],\n",
    "            \"dialog_state\": new_dialog_state,\n",
    "        }\n",
    "\n",
    "    return entry_node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff93003-6c61-437c-9510-4eeaecfd517b",
   "metadata": {},
   "source": [
    "#### Define Graph\n",
    "\n",
    "Now it's time to start building our graph. As before, we'll start with a node to pre-populate the state with the user's current information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c1140c-cd4e-4d69-bddd-7baa1eb4540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "\n",
    "def user_info(state: State):\n",
    "    return {\"user_info\": \"\"}\n",
    "\n",
    "\n",
    "builder.add_node(\"fetch_user_info\", user_info)\n",
    "builder.add_edge(START, \"fetch_user_info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fc80d0-fbf7-4631-9a8e-6b5af966e112",
   "metadata": {},
   "source": [
    "Now we'll start adding our specialized workflows. Each mini-workflow looks very similar to our full graph in [Part 3](#part-3-conditional-interrupt), employing 5 nodes:\n",
    "\n",
    "1. `enter_*`: use the `create_entry_node` utility you defined above to add a ToolMessage signaling that the new specialized assistant is at the helm\n",
    "2. Assistant: the prompt + llm combo that takes in the current state and either uses a tool, asks a question of the user, or ends the workflow (return to the primary assistant)\n",
    "3. `*_safe_tools`: \"read-only\" tools the assistant can use without user confirmation.\n",
    "4. `*_sensitive_tools`: tools with \"write\" access that require user confirmation (and will be assigned an `interrupt_before` when we compile the graph)\n",
    "5. `leave_skill`: _pop_ the `dialog_state` to signal that the _primary assistant_ is back in control\n",
    "\n",
    "Because of their similarities, we _could_ define a factory function to generate these. Since this is a tutorial, we'll define them each explicitly.\n",
    "\n",
    "First, make the **flight booking assistant** dedicated to managing the user journey for updating and canceling flights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c220e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entry Node for Analytics Assistant\n",
    "builder.add_node(\n",
    "    \"enter_analytics_assistant\",\n",
    "    create_entry_node(\"Healthcare Analytics Assistant\", \"analytics_assistant\"),\n",
    ")\n",
    "builder.add_node(\"analytics_assistant\", Assistant(update_anlaytics_runnable))\n",
    "builder.add_edge(\"enter_analytics_assistant\", \"analytics_assistant\")\n",
    "builder.add_node(\n",
    "    \"analytics_tools\",\n",
    "    create_tool_node_with_fallback(update_anlaytics_tools),\n",
    ")\n",
    "\n",
    "# Routing Logic\n",
    "def route_analytics_assistant(state: State):\n",
    "    route = tools_condition(state)\n",
    "    if route == END:\n",
    "        return END\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    did_cancel = any(tc[\"name\"] == CompleteOrEscalate.__name__ for tc in tool_calls)\n",
    "    if did_cancel:\n",
    "        return \"leave_skill\"\n",
    "    return \"analytics_tools\"\n",
    "\n",
    "# Edges for Analytics Assistant\n",
    "builder.add_edge(\"analytics_tools\", \"analytics_assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"analytics_assistant\",\n",
    "    route_analytics_assistant,\n",
    "    [\"analytics_tools\", \"leave_skill\", END],\n",
    ")\n",
    "\n",
    "# This node will be shared for exiting all specialized assistants\n",
    "def pop_dialog_state(state: State) -> dict:\n",
    "    \"\"\"Pop the dialog stack and return to the main assistant.\n",
    "\n",
    "    This lets the full graph explicitly track the dialog flow and delegate control\n",
    "    to specific sub-graphs.\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    if state[\"messages\"][-1].tool_calls:\n",
    "        # Note: Doesn't currently handle the edge case where the llm performs parallel tool calls\n",
    "        messages.append(\n",
    "            ToolMessage(\n",
    "                content=\"Resuming dialog with the host assistant. Please reflect on the past conversation and assist the user as needed.\",\n",
    "                tool_call_id=state[\"messages\"][-1].tool_calls[0][\"id\"],\n",
    "            )\n",
    "        )\n",
    "    return {\n",
    "        \"dialog_state\": \"pop\",\n",
    "        \"messages\": messages,\n",
    "    }\n",
    "\n",
    "\n",
    "builder.add_node(\"leave_skill\", pop_dialog_state)\n",
    "builder.add_edge(\"leave_skill\", \"primary_assistant\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706e40ee-2f75-4a5a-bfbc-233b0e7b7eb4",
   "metadata": {},
   "source": [
    "Next, create the **car rental assistant** graph to own all car rental needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68b93f5-0f72-4e94-8e8b-b501ec82edcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entry Node for Prospecting Assistant\n",
    "builder.add_node(\n",
    "    \"enter_prospecting_assistant\",\n",
    "    create_entry_node(\"Prospecting Assistant\", \"prospecting_assistant\"),\n",
    ")\n",
    "builder.add_node(\"prospecting_assistant\", Assistant(prospecting_runnable))\n",
    "builder.add_edge(\"enter_prospecting_assistant\", \"prospecting_assistant\")\n",
    "builder.add_node(\n",
    "    \"prospecting_tools\",\n",
    "    create_tool_node_with_fallback(prospecting_tools),\n",
    ")\n",
    "\n",
    "# Routing Logic\n",
    "def route_prospecting_assistant(state: State):\n",
    "    route = tools_condition(state)\n",
    "    if route == END:\n",
    "        return END\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    did_cancel = any(tc[\"name\"] == CompleteOrEscalate.__name__ for tc in tool_calls)\n",
    "    if did_cancel:\n",
    "        return \"leave_skill\"\n",
    "    return \"prospecting_tools\"\n",
    "\n",
    "# Edges for Prospecting Assistant\n",
    "builder.add_edge(\"prospecting_tools\", \"prospecting_assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"prospecting_assistant\",\n",
    "    route_prospecting_assistant,\n",
    "    [\"prospecting_tools\", \"leave_skill\", END],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e8aa17-8562-4fe8-9418-69703ec1946b",
   "metadata": {},
   "source": [
    "Then define the **hotel booking** workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40edb9-d415-4f43-8f9f-c82a239c607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entry Node for Lead Qualification Assistant\n",
    "builder.add_node(\n",
    "    \"enter_lead_qualification\",\n",
    "    create_entry_node(\"Lead Qualification Assistant\", \"lead_qualification_assistant\"),\n",
    ")\n",
    "builder.add_node(\"lead_qualification_assistant\", Assistant(lead_qualification_runnable))\n",
    "builder.add_edge(\"enter_lead_qualification\", \"lead_qualification_assistant\")\n",
    "builder.add_node(\n",
    "    \"lead_qualification_tools\",\n",
    "    create_tool_node_with_fallback(book_lead_qualification_tools),\n",
    ")\n",
    "\n",
    "# Routing Logic\n",
    "def route_lead_qualification(state: State):\n",
    "    route = tools_condition(state)\n",
    "    if route == END:\n",
    "        return END\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    did_cancel = any(tc[\"name\"] == CompleteOrEscalate.__name__ for tc in tool_calls)\n",
    "    if did_cancel:\n",
    "        return \"leave_skill\"\n",
    "    return \"lead_qualification_tools\"\n",
    "\n",
    "# Edges for Lead Qualification Assistant\n",
    "builder.add_edge(\"lead_qualification_tools\", \"lead_qualification_assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"lead_qualification_assistant\",\n",
    "    route_lead_qualification,\n",
    "    [\"lead_qualification_tools\", \"leave_skill\", END],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c40aa1-b820-4d0a-9c08-76a8ad16044b",
   "metadata": {},
   "source": [
    "After that, define the **excursion assistant**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce9cf21-f708-4033-bca6-5f5d110b5662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entry Node for Strategy Planner Assistant\n",
    "builder.add_node(\n",
    "    \"enter_strategy_planner\",\n",
    "    create_entry_node(\"Strategy Planner Assistant\", \"strategy_planner_assistant\"),\n",
    ")\n",
    "builder.add_node(\"strategy_planner_assistant\", Assistant(strategy_runnable))\n",
    "builder.add_edge(\"enter_strategy_planner\", \"strategy_planner_assistant\")\n",
    "builder.add_node(\n",
    "    \"strategy_tools\",\n",
    "    create_tool_node_with_fallback(strategy_tools),\n",
    ")\n",
    "\n",
    "# Routing Logic\n",
    "def route_strategy_planner(state: State):\n",
    "    route = tools_condition(state)\n",
    "    if route == END:\n",
    "        return END\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    did_cancel = any(tc[\"name\"] == CompleteOrEscalate.__name__ for tc in tool_calls)\n",
    "    if did_cancel:\n",
    "        return \"leave_skill\"\n",
    "    return \"strategy_tools\"\n",
    "\n",
    "# Edges for Strategy Planner Assistant\n",
    "builder.add_edge(\"strategy_tools\", \"strategy_planner_assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"strategy_planner_assistant\",\n",
    "    route_strategy_planner,\n",
    "    [ \"strategy_tools\", \"leave_skill\", END],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd73cdd-e50f-4819-82a4-d867359f9bb6",
   "metadata": {},
   "source": [
    "Finally, create the **primary assistant**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acb19faf-66c8-4fd8-89ec-4d97d510ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.runnables import Runnable\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "# Primary Assistant Node\n",
    "builder.add_node(\"primary_assistant\", Assistant(assistant_runnable))\n",
    "builder.add_node(\n",
    "    \"primary_assistant_tools\", create_tool_node_with_fallback(primary_assistant_tools)\n",
    ")\n",
    "\n",
    "# Routing Logic for Specialized Assistants\n",
    "def route_primary_assistant(state: State):\n",
    "    \"\"\"\n",
    "    Route tasks to the appropriate specialized assistant or tools based on tool calls.\n",
    "    \"\"\"\n",
    "    route = tools_condition(state)\n",
    "    if route == END:\n",
    "        return END\n",
    "\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    if tool_calls:\n",
    "        if tool_calls[0][\"name\"] == ToAnalyticsAssistant.__name__:\n",
    "            return \"enter_analytics_assistant\"\n",
    "        elif tool_calls[0][\"name\"] == ToProspectingAssistant.__name__:\n",
    "            return \"enter_prospecting_assistant\"\n",
    "        elif tool_calls[0][\"name\"] == ToLeadQualification.__name__:\n",
    "            return \"enter_lead_qualification\"\n",
    "        elif tool_calls[0][\"name\"] == ToStrategyAssistant.__name__:\n",
    "            return \"enter_strategy_planner\"\n",
    "        return \"primary_assistant_tools\"\n",
    "    raise ValueError(\"Invalid route\")\n",
    "\n",
    "# Conditional Edges for Routing\n",
    "builder.add_conditional_edges(\n",
    "    \"primary_assistant\",\n",
    "    route_primary_assistant,\n",
    "    [\n",
    "        \"enter_analytics_assistant\",\n",
    "        \"enter_prospecting_assistant\",\n",
    "        \"enter_lead_qualification\",\n",
    "        \"enter_strategy_planner\",\n",
    "        \"primary_assistant_tools\",\n",
    "        END,\n",
    "    ],\n",
    ")\n",
    "builder.add_edge(\"primary_assistant_tools\", \"primary_assistant\")\n",
    "\n",
    "# Route Back to Current Workflow\n",
    "def route_to_workflow(\n",
    "    state: State,\n",
    ") -> Literal[\n",
    "    \"primary_assistant\",\n",
    "    \"analytics_assistant\",\n",
    "    \"prospecting_assistant\",\n",
    "    \"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    lead_qualification_assistant\",\n",
    "    \"strategy_planner_assistant\",\n",
    "]:\n",
    "    \"\"\"\n",
    "    Return to the currently active assistant workflow based on dialog state.\n",
    "    \"\"\"\n",
    "    dialog_state = state.get(\"dialog_state\")\n",
    "    if not dialog_state:\n",
    "        return \"primary_assistant\"\n",
    "    return dialog_state[-1]\n",
    "\n",
    "builder.add_conditional_edges(\"fetch_user_info\", route_to_workflow)\n",
    "\n",
    "# Compile Graph\n",
    "memory = MemorySaver()\n",
    "part_4_graph = builder.compile(\n",
    "    checkpointer=memory,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a6f01e-4779-45e3-9e18-376cf05c6065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(part_4_graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3eb142-709d-4c29-9dbf-a34c5e800343",
   "metadata": {},
   "source": [
    "#### Conversation\n",
    "\n",
    "That was a lot! Let's run it over the following list of dialog turns. This time, we'll have many fewer confirmations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef6b495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a548d-029a-47b7-9ac0-9c5203ec92c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import uuid\n",
    "# Let's create an example conversation a user might have with the assistant\n",
    "tutorial_questions = [\n",
    "    \"Can you provide a list of top cardiologists in New York City who might be interested in advanced diagnostic tools?Can you provide a list of top cardiologists in New York City who might be interested in advanced diagnostic tools?\",\n",
    "    # \"List the decision-makers in the procurement departments of major hospitals in Texas.\",\n",
    "    # \"Who are the leading oncologists specializing in telemedicine?\",\n",
    "    # \"Identify hospitals that have recently invested in health tech solutions for patient monitoring.\",\n",
    "    # \"Which healthcare organizations are focused on adopting AI technologies?\"\n",
    "]\n",
    "# Update with the backup file so we can restart from the original place in each section\n",
    "# db = update_dates(db)\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # The passenger_id is used in our flight tools to\n",
    "        # fetch the user's flight information\n",
    "        \"passenger_id\": \"3442 587242\",\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}\n",
    "\n",
    "_printed = set()\n",
    "# We can reuse the tutorial questions from part 1 to see how it does.\n",
    "for question in tutorial_questions:\n",
    "    events = part_4_graph.stream(\n",
    "        {\"messages\": (\"user\", question)}, config, stream_mode=\"values\"\n",
    "    )\n",
    "    for event in events:\n",
    "        _print_event(event, _printed)\n",
    "    snapshot = part_4_graph.get_state(config)\n",
    "    while snapshot.next:\n",
    "        # We have an interrupt! The agent is trying to use a tool, and the user can approve or deny it\n",
    "        # Note: This code is all outside of your graph. Typically, you would stream the output to a UI.\n",
    "        # Then, you would have the frontend trigger a new run via an API call when the user has provided input.\n",
    "        try:\n",
    "            user_input = input(\n",
    "                \"Do you approve of the above actions? Type 'y' to continue;\"\n",
    "                \" otherwise, explain your requested changed.\\n\\n\"\n",
    "            )\n",
    "        except:\n",
    "            user_input = \"y\"\n",
    "        if user_input.strip() == \"y\":\n",
    "            # Just continue\n",
    "            result = part_4_graph.invoke(\n",
    "                None,\n",
    "                config,\n",
    "            )\n",
    "        else:\n",
    "            # Satisfy the tool invocation by\n",
    "            # providing instructions on the requested changes / change of mind\n",
    "            result = part_4_graph.invoke(\n",
    "                {\n",
    "                    \"messages\": [\n",
    "                        ToolMessage(\n",
    "                            tool_call_id=event[\"messages\"][-1].tool_calls[0][\"id\"],\n",
    "                            content=f\"API call denied by user. Reasoning: '{user_input}'. Continue assisting, accounting for the user's input.\",\n",
    "                        )\n",
    "                    ]\n",
    "                },\n",
    "                config,\n",
    "            )\n",
    "        snapshot = part_4_graph.get_state(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764f2c09-d9ff-4f14-8507-5018c17edbb3",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "\n",
    "You've now developed a customer support bot that handles diverse tasks using focused workflows.\n",
    "More importantly, you've learned to use some of LangGraph's core features to design and refactor an application based on your product needs.\n",
    "\n",
    "The above examples are by no means optimized for your unique needs - LLMs make mistakes, and each flow can be made more reliable through better prompts and experimentation. Once you've created your initial support bot, the next step would be to start [adding evaluations](https://docs.smith.langchain.com/evaluation) so you can confidently improve your system. Check out those docs and our other tutorials to learn more!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
